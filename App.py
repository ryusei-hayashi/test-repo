from statistics import fmean, mean
from tensorflow import keras
from yt_dlp import YoutubeDL
import tensorflow_probability as tfp
import tensorflow as tf
import streamlit as st
import requests
import spotipy
import librosa
import pandas
import numpy

st.set_page_config(page_title='Test App', page_icon='ðŸŽµ', layout='wide')

yd = YoutubeDL({'outtmpl': 'music', 'playlist_items': '1', 'format': 'mp3/bestaudio/best', 'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3'}], 'overwrites': True})
sp = spotipy.Spotify(spotipy.oauth2.SpotifyClientCredentials(st.secrets['id'], st.secrets['pw']))
sr = 22050
fps = 25
sec = 10
seq = 256
z_n = 32
x_n = 1024

@st.cache_resource
def load_h5(w):
    m = VAE()
    m(tf.random.normal([1, x_n, seq, 1]))
    m.load_weights(w)
    return m

@st.cache_data
def load_np(f):
    return numpy.load(f, allow_pickle=True).item()

@st.cache_data
def download(n):
    try:
        if m == 'YouTubeDL':
            yd.download([n])
        elif m == 'Spotify API':
            open('music.mp3', 'wb').write(requests.get(f'{sp.track(n.replace("intl-ja/", ""))["preview_url"]}.mp3').content)
        elif m == 'Audiostock':
            open('music.mp3', 'wb').write(requests.get(f'{n}/play.mp3').content)
        elif m == 'Uploader':
            open('music.mp3', 'wb').write(n.getbuffer())
        st.audio('music.mp3')
    except:
        st.error(f'Error: Unable to access {n}')

@st.cache_data
def extract(s, v, a):
    return [k for k in Z if all(i in S[k] for i in s) and v[0] < V[k][0] < v[1] and a[0] < V[k][1] < a[1]]

@st.cache_data
def center(K):
    return numpy.mean(numpy.array([Z[k] for k in K]), axis=0)
    
def trim(y):
    b = librosa.beat.beat_track(y=y, sr=sr, hop_length=sr//fps)[1]
    if len(b) < 9:
        y[:sr*sec]
    s = mean(b[:2])
    i = numpy.searchsorted(b, s + sec * fps) - 1
    return y[sr*s//fps:sr*mean(b[i:i+2])//fps]

def stft(y):
    return librosa.magphase(librosa.stft(y=y, hop_length=sr//fps, n_fft=2*x_n-1))[0]

def cqt(y):
    return librosa.magphase(librosa.cqt(y=y, hop_length=sr//fps, n_bins=x_n, bins_per_octave=x_n//7))[0]

def mel(y):
    return librosa.feature.melspectrogram(y=y, hop_length=sr//fps, n_mels=x_n)

def pad(y):
    return numpy.pad(y, ((0, x_n-y.shape[0]), (0, seq-y.shape[1])), constant_values=-1e-300)

def collate(X):
    Y = numpy.empty((len(X), x_n, seq), numpy.float32)
    for i, x in enumerate(X):
        y = librosa.load(x, sr=sr, offset=10, duration=2*sec)[0]
        Y[i] = pad(stft(trim(y))[:x_n,:seq])
    return Y[:,:,:,numpy.newaxis]

class Conv1(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(Conv1, self).__init__()
        self.cv = keras.layers.Conv2D(channel, kernel, stride, padding)
        self.bn = keras.layers.BatchNormalization()

    def call(self, x):
        return tf.nn.relu(self.cv(self.bn(x)))

class ConvT1(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(ConvT1, self).__init__()
        self.cv = keras.layers.Conv2DTranspose(channel, kernel, stride, padding)
        self.bn = keras.layers.BatchNormalization()

    def call(self, x):
        return tf.nn.relu(self.cv(self.bn(x)))

class Conv2(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(Conv2, self).__init__()
        self.cv1 = Conv1(channel, kernel, stride, padding)
        self.cv2 = Conv1(channel, kernel, stride, padding)

    def call(self, x):
        return self.cv2(self.cv1(x))

class ConvT2(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(ConvT2, self).__init__()
        self.cvt1 = ConvT1(channel, kernel, stride, padding)
        self.cvt2 = ConvT1(channel, kernel, stride, padding)

    def call(self, x):
        return self.cvt2(self.cvt1(x))

class Conv5(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(Conv5, self).__init__()
        self.cv1 = Conv1(channel[0], (kernel[0], 1), (stride[0], 1), padding[0])
        self.cv2 = Conv2(channel[0], (1, kernel[1]), (1, stride[1]), padding[1])
        self.cv3 = Conv2(channel[1], (1, kernel[1]), (1, stride[1]), padding[1])

    def call(self, x, y):
        return self.cv3(x), self.cv2(tf.nn.relu(self.cv1(x) + y))

class ConvT5(keras.Model):
    def __init__(self, channel, kernel, stride, padding):
        super(ConvT5, self).__init__()
        self.cvt1 = ConvT1(channel[0], (kernel[0], 1), (stride[0], 1), padding[0])
        self.cvt2 = ConvT2(channel[0], (1, kernel[1]), (1, stride[1]), padding[1])
        self.cvt3 = ConvT2(channel[1], (1, kernel[1]), (1, stride[1]), padding[1])

    def call(self, x, y):
        return self.cvt2(tf.nn.relu(self.cvt1(y) + x)), self.cvt3(y)

class Encoder(keras.Model):
    def __init__(self, a_n, v_n):
        super(Encoder, self).__init__()
        self.cv1 = keras.layers.Conv2D(a_n, (1, 1), activation='relu')
        self.cv2 = Conv5((a_n + v_n, a_n), (x_n, 8), (1, 4), ('valid', 'same'))
        self.cv3 = Conv5((a_n + v_n, a_n), (x_n, 8), (1, 4), ('valid', 'same'))
        self.cv4 = keras.layers.Conv2D(a_n + v_n, (x_n, 1), activation='relu')
        self.fc1 = keras.layers.Dense(a_n + v_n)
        self.fc2 = keras.layers.Dense(a_n + v_n)

    def call(self, x):
        x = self.cv1(x)
        x, y = self.cv2(x, 0.0)
        x, y = self.cv3(x, y)
        x = self.cv4(x)
        y = tf.nn.relu(x + y)
        y = tf.reshape(y, (-1, y.shape[-1]))
        y = self.fc1(y)
        y = tf.nn.relu(y)
        y = self.fc2(y)
        return y

class Decoder(keras.Model):
    def __init__(self, a_n, v_n):
        super(Decoder, self).__init__()
        self.fc1 = keras.layers.Dense(a_n + v_n)
        self.fc2 = keras.layers.Dense(a_n + v_n)
        self.cvt1 = ConvT5((a_n, a_n + v_n), (x_n, 8), (1, 4), ('valid', 'same'))
        self.cvt2 = ConvT5((a_n, a_n + v_n), (x_n, 8), (1, 4), ('valid', 'same'))
        self.cvt3 = keras.layers.Conv2DTranspose(a_n, (x_n, 1), activation='relu')
        self.cvt4 = keras.layers.Conv2DTranspose(1, (1, 1), activation='relu')

    def call(self, z):
        y = self.fc1(z)
        y = tf.nn.relu(y)
        y = self.fc2(y)
        y = tf.reshape(y, (-1, 1, 1, y.shape[-1]))
        x, y = self.cvt1(0.0, y)
        x, y = self.cvt2(x, y)
        y = self.cvt3(y)
        x = tf.nn.relu(x + y)
        x = self.cvt4(x)
        return x

class VAE(keras.Model):
    def __init__(self):
        super().__init__()
        self.encoder = Encoder(z_n, z_n * (z_n + 1) // 2)
        self.decoder = Decoder(z_n, z_n * (z_n + 1) // 2)
        self.sample = tfp.layers.MultivariateNormalTriL(z_n, activity_regularizer=tfp.layers.KLDivergenceRegularizer(tfp.distributions.Independent(tfp.distributions.Normal(tf.zeros(z_n), 1), 1), weight=1e-4))

    def call(self, x):
        x = self.encoder(x)
        z = self.sample(x)
        y = self.decoder(z)
        return y

    def get_z(self, x):
        return tf.convert_to_tensor(self.sample(self.encoder(x, training=False))).numpy()
    
M = load_h5('data/vae.h5')
S = load_np('data/scn.npy')
V = load_np('data/vad.npy')
Z = load_np('data/vec.npy')
U = load_np('data/url.npy')

st.title('App')
st.write('This application retrieves music that has both the worldview of the game and the atmosphere of the scene.')

st.subheader('Input Music')
m = st.selectbox('Input Method', ['YouTubeDL', 'Audiostock', 'Uploader'])
if m == 'Uploader':
    n = st.file_uploader('Upload File')
else:
    n = st.text_input('Input URL')
if n:
    download(n)

l, r = st.columns(2, gap='medium')
with l:
    st.subheader('Scene of Input Music')
    sim = st.multiselect('State of input music', ['ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°', 'ã‚¿ã‚¤ãƒˆãƒ«', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«', 'ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼', 'ã‚²ãƒ¼ãƒ ã‚¯ãƒªã‚¢', 'ã‚»ãƒ¬ã‚¯ãƒˆ', 'ã‚·ãƒ§ãƒƒãƒ—', 'ãƒŸãƒ‹ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚»ãƒ¼ãƒ•ã‚¾ãƒ¼ãƒ³', 'ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒžãƒƒãƒ—', 'ãƒ€ãƒ³ã‚¸ãƒ§ãƒ³', 'ã‚¹ãƒ†ãƒ¼ã‚¸', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'])
    tim = st.multiselect('Time of input music', ['æ˜¥', 'å¤', 'ç§‹', 'å†¬', 'æœ', 'æ˜¼', 'å¤œ', 'å¤•æ–¹', 'ä¼‘æ—¥', 'å¤ä»£', 'ä¸­ä¸–', 'è¿‘ä»£', 'ç¾ä»£', 'æœªæ¥'])
    wim = st.multiselect('Weather of input music', ['æ™´ã‚Œ', 'è™¹', 'é›²', 'åµ', 'é›ª', 'ç ‚', 'é›¨', 'å°é›¨', 'æ··æ²Œ'])
    bim = st.multiselect('Biome of input music', ['æ°´ä¸Š', 'æ°´ä¸­', 'æµ·', 'æ¹–', 'å·', 'å±±', 'å³¶', 'æµœè¾º', 'æ´žçªŸ', 'ç ‚æ¼ ', 'è’é‡Ž', 'è‰åŽŸ', 'ç†±å¸¯', 'æ£®', 'ç‚Ž', 'ç©º', 'å®‡å®™', 'ç•°æ¬¡å…ƒ'])
    pim = st.multiselect('Place of input music', ['ä»®æƒ³ç¾å®Ÿ', 'å¤–å›½', 'éƒ½ä¼š', 'ç”°èˆŽ', 'è¡—', 'ã‚¢ã‚¸ãƒˆ', 'ã‚ªãƒ•ã‚£ã‚¹', 'ãƒ“ãƒ«', 'ã‚¸ãƒ ', 'è¾²åœ°', 'ç‰§å ´', 'å·¥å ´', 'ç ”ç©¶æ‰€', 'è»äº‹åŸºåœ°', 'å­¦æ ¡', 'å…¬åœ’', 'ç—…é™¢', 'æ³•å»·', 'ç«¶æŠ€å ´', 'ç¾Žè¡“é¤¨', 'é£›è¡Œæ©Ÿ', 'é›»è»Š', 'èˆ¹', 'æ©‹', 'ã‚·ã‚¢ã‚¿ãƒ¼', 'ã‚«ã‚¸ãƒŽ', 'éŠåœ’åœ°', 'åŸŽ', 'éºè·¡', 'ç¥žç¤¾', 'å¯ºé™¢', 'æ•™ä¼š', 'å®®æ®¿', 'ç¥žæ®¿', 'è–åŸŸ', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'ãƒ›ãƒ†ãƒ«', 'ãƒãƒ¼', 'é…’å ´', 'åº—', 'å®¶', 'å»ƒå¢Ÿ', 'é«˜å°'])
    qim = st.multiselect('Person of input music', ['ä¸»äººå…¬', 'ç›¸æ£’', 'ä»²é–“', 'å…ˆäºº', 'è¦³è¡†', 'æ—¥å¸¸', 'éžå¸¸', 'æ•µ', 'å­¤ç‹¬', 'è£åˆ‡è€…', 'ä¸­ãƒœã‚¹', 'ãƒ©ã‚¹ãƒœã‚¹', 'ãƒ©ã‚¤ãƒãƒ«', 'ãƒžã‚¹ã‚³ãƒƒãƒˆ', 'ãƒ’ãƒ­ã‚¤ãƒ³', 'ãƒ¢ãƒ–'])
    aim = st.multiselect('Action of input music', ['ç§»å‹•', 'èµ°ã‚‹', 'æ³³ã', 'é£›ã¶', 'é‹å‹•', 'ç«¶èµ°', 'éŠã¶', 'ä¼‘ã‚€', 'è€ƒãˆã‚‹', 'é–ƒã', 'ä½œæ¥­', 'æˆ¦ã†', 'æ½œå…¥', 'æŽ¢ç´¢', 'è¿½ã†', 'é€ƒã’ã‚‹', 'å–å¼•ã', 'å®´', 'å‹åˆ©', 'å›žæƒ³', 'è¦šé†’', 'æ„Ÿå‹•', 'èª¬å¾—', 'æ±ºæ„', 'æˆé•·', 'æ‚©ã‚€', 'å‡ºä¼šã„', 'åˆ¥ã‚Œ', 'ç™»å ´', 'ä¸ç©', 'å¹³ç©', 'è§£èª¬', 'ç†±ç‹‚', 'å›°æƒ‘', 'è¬€ç•¥', 'çŠ¯ç½ª', 'æš´åŠ›', 'ãµã–ã‘ã‚‹', 'ã‚ãŠã‚‹', 'æ‹æ„›', 'æ„Ÿè¬', 'ç™’ã™', 'åŠ±ã¾ã™', 'å‡ºæŽ›ã‘ã‚‹'])
    vim = st.slider('Valence of input music', -1.0, 1.0, (-1.0, 1.0))
    zim = st.slider('Arousal of input music', -1.0, 1.0, (-1.0, 1.0))
with r:
    st.subheader('Scene of Output Music')
    som = st.multiselect('State of output music', ['ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°', 'ã‚¿ã‚¤ãƒˆãƒ«', 'ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«', 'ã‚²ãƒ¼ãƒ ã‚ªãƒ¼ãƒãƒ¼', 'ã‚²ãƒ¼ãƒ ã‚¯ãƒªã‚¢', 'ã‚»ãƒ¬ã‚¯ãƒˆ', 'ã‚·ãƒ§ãƒƒãƒ—', 'ãƒŸãƒ‹ã‚¤ãƒ™ãƒ³ãƒˆ', 'ã‚»ãƒ¼ãƒ•ã‚¾ãƒ¼ãƒ³', 'ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒžãƒƒãƒ—', 'ãƒ€ãƒ³ã‚¸ãƒ§ãƒ³', 'ã‚¹ãƒ†ãƒ¼ã‚¸', 'ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°'])
    tom = st.multiselect('Time of output music', ['æ˜¥', 'å¤', 'ç§‹', 'å†¬', 'æœ', 'æ˜¼', 'å¤œ', 'å¤•æ–¹', 'ä¼‘æ—¥', 'å¤ä»£', 'ä¸­ä¸–', 'è¿‘ä»£', 'ç¾ä»£', 'æœªæ¥'])
    wom = st.multiselect('Weather of output music', ['æ™´ã‚Œ', 'è™¹', 'é›²', 'åµ', 'é›ª', 'ç ‚', 'é›¨', 'å°é›¨', 'æ··æ²Œ'])
    bom = st.multiselect('Biome of output music', ['æ°´ä¸Š', 'æ°´ä¸­', 'æµ·', 'æ¹–', 'å·', 'å±±', 'å³¶', 'æµœè¾º', 'æ´žçªŸ', 'ç ‚æ¼ ', 'è’é‡Ž', 'è‰åŽŸ', 'ç†±å¸¯', 'æ£®', 'ç‚Ž', 'ç©º', 'å®‡å®™', 'ç•°æ¬¡å…ƒ'])
    pom = st.multiselect('Place of output music', ['ä»®æƒ³ç¾å®Ÿ', 'å¤–å›½', 'éƒ½ä¼š', 'ç”°èˆŽ', 'è¡—', 'ã‚¢ã‚¸ãƒˆ', 'ã‚ªãƒ•ã‚£ã‚¹', 'ãƒ“ãƒ«', 'ã‚¸ãƒ ', 'è¾²åœ°', 'ç‰§å ´', 'å·¥å ´', 'ç ”ç©¶æ‰€', 'è»äº‹åŸºåœ°', 'å­¦æ ¡', 'å…¬åœ’', 'ç—…é™¢', 'æ³•å»·', 'ç«¶æŠ€å ´', 'ç¾Žè¡“é¤¨', 'é£›è¡Œæ©Ÿ', 'é›»è»Š', 'èˆ¹', 'æ©‹', 'ã‚·ã‚¢ã‚¿ãƒ¼', 'ã‚«ã‚¸ãƒŽ', 'éŠåœ’åœ°', 'åŸŽ', 'éºè·¡', 'ç¥žç¤¾', 'å¯ºé™¢', 'æ•™ä¼š', 'å®®æ®¿', 'ç¥žæ®¿', 'è–åŸŸ', 'ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³', 'ã‚«ãƒ•ã‚§', 'ãƒ›ãƒ†ãƒ«', 'ãƒãƒ¼', 'é…’å ´', 'åº—', 'å®¶', 'å»ƒå¢Ÿ', 'é«˜å°'])
    qom = st.multiselect('Person of output music', ['ä¸»äººå…¬', 'ç›¸æ£’', 'ä»²é–“', 'å…ˆäºº', 'è¦³è¡†', 'æ—¥å¸¸', 'éžå¸¸', 'æ•µ', 'å­¤ç‹¬', 'è£åˆ‡è€…', 'ä¸­ãƒœã‚¹', 'ãƒ©ã‚¹ãƒœã‚¹', 'ãƒ©ã‚¤ãƒãƒ«', 'ãƒžã‚¹ã‚³ãƒƒãƒˆ', 'ãƒ’ãƒ­ã‚¤ãƒ³', 'ãƒ¢ãƒ–'])
    aom = st.multiselect('Action of output music', ['ç§»å‹•', 'èµ°ã‚‹', 'æ³³ã', 'é£›ã¶', 'é‹å‹•', 'ç«¶èµ°', 'éŠã¶', 'ä¼‘ã‚€', 'è€ƒãˆã‚‹', 'é–ƒã', 'ä½œæ¥­', 'æˆ¦ã†', 'æ½œå…¥', 'æŽ¢ç´¢', 'è¿½ã†', 'é€ƒã’ã‚‹', 'å–å¼•ã', 'å®´', 'å‹åˆ©', 'å›žæƒ³', 'è¦šé†’', 'æ„Ÿå‹•', 'èª¬å¾—', 'æ±ºæ„', 'æˆé•·', 'æ‚©ã‚€', 'å‡ºä¼šã„', 'åˆ¥ã‚Œ', 'ç™»å ´', 'ä¸ç©', 'å¹³ç©', 'è§£èª¬', 'ç†±ç‹‚', 'å›°æƒ‘', 'è¬€ç•¥', 'çŠ¯ç½ª', 'æš´åŠ›', 'ãµã–ã‘ã‚‹', 'ã‚ãŠã‚‹', 'æ‹æ„›', 'æ„Ÿè¬', 'ç™’ã™', 'åŠ±ã¾ã™', 'å‡ºæŽ›ã‘ã‚‹'])
    vom = st.slider('Valence of output music', -1.0, 1.0, (-1.0, 1.0))
    zom = st.slider('Arousal of output music', -1.0, 1.0, (-1.0, 1.0))

st.subheader('Output Music')
if st.button('Retrieve'):
    P = extract(sim + tim + wim + bim + pim + qim + aim, vim, zim)
    Q = extract(som + tom + wom + bom + pom + qom + aom, vom, zom)
    z = M.get_z(collate(['music.mp3']))[0] + center(Q) - center(P)
    D = pandas.DataFrame([U[k] for k in sorted(Q, key=lambda k: numpy.linalg.norm(Z[k]-z))[:99]], columns=['URL', 'Name', 'Artist', 'Time'])
    st.dataframe(D, column_config={'URL': st.column_config.LinkColumn()})
